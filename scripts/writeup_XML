Note on XML Evaluation

Scenarios:
P1 - Convert Textbox into table,list or textbox depending on the size, font and color of the text inside the textbox
P2 - Proper alignment of Images: if 1 image, then align center; if 2 images then align them right and left; if 3 images then align them left, center, right; if more than 3 images then leave them as it is
P3 - Change bullet colors in list depending on the color of the bullet itself and the color of the neighbour bullets
P4 - Change color,size of text in slide depending on the alignment of the image inside the slide
P5 - Change text style(bold, underline, italic) in textbox depending on font,size of text
P6 - Convert List into table, textbox or list depending on bullettype, size and font of text in list
P7 - Change text indentation in table depending on the border, font of text inside table. 

Dataset Generation:
For each of the cases, synthetic data is randomly generated in the following manner:

For all scenarios and all cases, the training dataset (which is used for clustering) contains 100 examples, tuning data (which is used for learning weights for features) contains 300 examples and test dataset contains 100 examples. Each example in all cases has around 3-4 attributes with each atrribute having on average 4-5 values. The values for the attributes are chosen randomly from a predescribed set of values.  A datapoint is said to be a confusion datapoint if it belongs to more than 1 cluster.

We first generate unconfused data points(data points which uniquely belong to a cluster) and append (input,output) to a list. Confusion datapoints are then generated by carefully generating points which belong to more than one cluster. For confusion datapoints, the output belongs to a unique output cluster given the datapoint follows a fixed constraint. This (input,output) is appended to the previous list. The list is then shuffled randomly in order to ensure that the order of examples does not effect our results. 

This approach is taken to generate all the datasets with a given confusion ratio. In all the cases in all scenarios, the test dataset has 10% confusion. Note that, learning to rank is thus helpful since it helps us to judge how much a confusion datapoint in test follows the constraint and assign it to the appropriate input cluster.

Observations:
For all scenario's, it is expected that the accuracy will first increase with increasing confusion upto a certain threshold and start decreasing further due to more conflicting examples. However, this threshold will depend on the dataset itself. 

For scenarios P1,P4,P5,P6 and P7 it was observed that the accuracy at 20% confusion > accuracy at 40% confusion > accuracy at 60% confusion. For scenario P3, it was observed that accuracy at 60% > accuracy at 40% > accuracy at 20%. The case of scenario P3 is special since total subsumption of input cluster LGGs' is taking place here. This implies that the level of confusion is much more than in the previous scenarios and is an indicator of the fact that threshold for this scenario would be significantly greater than that of previous datasets. For the scenario P2, oddly irrespectice of level of confusion all the examples outputs are obtained correctly. This may be due to the fact that ....

Special case of 40% confusion + 20% unseen points:
Unseen points are those points which do not belong to any input cluster completely. Unseen points are generated by assigning a particular attribute a value which has not been used while generating train dataset (which is used for clustering). We have taken a simple case where all unseen points go to a single output cluster but get partially subsumed with all input clusters.

For this simple case, in all scenarios it is observed that the accuracy achieved is better than accuracy achieved in 60% confusion but not at par with accuracy achieved in 40% confusion.    

For this special case, in all scenarios it is observed that the accuracy achieved is better than accuracy achieved in 60% confusion but not at par with accuracy achieved in 40% confusion. Note than though the unseen points do not belong to any one cluster completely, but they are uniquely assigned to an output cluster. As such, when learning the weights in tuning, the weights are appropriately tuned so as the unseen points are correctly assigned.  